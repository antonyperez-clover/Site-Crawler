name: Weekly Clover Docs Check

on:
  schedule:
    - cron: "0 2 * * 0" # Runs every Sunday at 2 AM UTC
  workflow_dispatch: # Allows manual triggering

jobs:
  run_crawler:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Needed to commit results back to the repo
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set current date as an environment variable
        run: echo "CURRENT_DATE=$(date +'%Y-%m-%d')" >> $GITHUB_ENV

      - name: Create output directories
        run: |
          mkdir -p docs
          mkdir -p crawler-temp-storage

      - name: Download and Run SiteOne Crawler
        id: crawl
        run: |
          LATEST_TAG=$(curl -s "https://api.github.com/repos/janreges/siteone-crawler/releases/latest" | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/')
          if [ -z "$LATEST_TAG" ]; then
            echo "Failed to fetch latest version tag from GitHub API. Exiting."
            exit 1
          fi
          FILENAME="siteone-crawler-${LATEST_TAG}-linux-x64.tar.gz"
          DOWNLOAD_URL="https://github.com/janreges/siteone-crawler/releases/download/${LATEST_TAG}/${FILENAME}"
          curl -L -o "$FILENAME" "$DOWNLOAD_URL"
          EXTRACTED_DIR=$(tar -tzf "$FILENAME" | head -1 | cut -f1 -d"/")
          tar -xzvf "$FILENAME"
          if [ ! -d "$EXTRACTED_DIR" ]; then
            echo "Extraction failed or the directory '$EXTRACTED_DIR' was not found. Exiting."
            exit 1
          fi
          cd "$EXTRACTED_DIR"
          ./crawler \
            --url="https://docs.clover.com" \
            --output-html-report="../docs/index.html" \
            --result-storage=file \
            --result-storage-dir="../crawler-temp-storage" \
            --workers=3 \
            --disable-javascript \ 
            --max-reqs-per-sec=5 \
            --timeout=15 \
            --max-visited-urls=10000 \
            --ignore-robots-txt=false
        continue-on-error: true

      - name: Inject Date into Report
        if: steps.crawl.outcome == 'success' && hashFiles('docs/index.html') != ''
        run: |
          # This command adds a title with the date at the top of the HTML body
          sed -i 's|<body>|<body>\n<h1 style="text-align:center;font-family:sans-serif;">Report Generated on ${{ env.CURRENT_DATE }}</h1>|' ./docs/index.html

      - name: Commit and push results
        run: |
          # Only proceed if the report exists
          if [ ! -f "./docs/index.html" ]; then
            echo "index.html not found. Skipping commit."
            exit 0
          fi
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add ./docs/index.html
          # Check if there are changes to commit before proceeding
          if ! git diff --staged --quiet; then
            git commit -m "Weekly crawl results for ${{ env.CURRENT_DATE }}"
            git push
          else
            echo "No changes in the report to commit."
          fi
