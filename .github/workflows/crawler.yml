name: Weekly Clover Docs Check

on:
  schedule:
    - cron: "0 2 * * 0" # Runs every Sunday at 2 AM UTC
  workflow_dispatch: # Allows manual triggering

jobs:
  run_crawler:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Needed to commit results back to the repo
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Create output directories
        run: |
          mkdir -p crawler-results
          mkdir -p crawler-temp-storage

      - name: Download and Run SiteOne Crawler
        id: crawl
        run: |
          # Step 1: Get the latest version tag from the GitHub API
          LATEST_TAG=$(curl -s "https://api.github.com/repos/janreges/siteone-crawler/releases/latest" | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/')
          if [ -z "$LATEST_TAG" ]; then
            echo "Failed to fetch latest version tag from GitHub API. Exiting."
            exit 1
          fi
          echo "Latest version found: $LATEST_TAG"

          # Step 2: Construct the download URL and filename for the Linux x64 version
          FILENAME="siteone-crawler-${LATEST_TAG}-linux-x64.tar.gz"
          DOWNLOAD_URL="https://github.com/janreges/siteone-crawler/releases/download/${LATEST_TAG}/${FILENAME}"

          # Step 3: Download the package
          echo "Downloading crawler from $DOWNLOAD_URL"
          curl -L -o "$FILENAME" "$DOWNLOAD_URL"

          # Step 4: Extract the archive and get the name of the folder it creates
          EXTRACTED_DIR=$(tar -tzf "$FILENAME" | head -1 | cut -f1 -d"/")
          tar -xzvf "$FILENAME"
          if [ ! -d "$EXTRACTED_DIR" ]; then
            echo "Extraction failed or the directory '$EXTRACTED_DIR' was not found. Exiting."
            exit 1
          fi
          echo "Crawler extracted to directory: $EXTRACTED_DIR"

          # Step 5: Run the crawler from within the new directory
          cd "$EXTRACTED_DIR"
          ./crawler \
            --url="https://docs.clover.com" \
            --output-html-report="../crawler-results/clover_docs_report.html" \
            --result-storage=file \
            --result-storage-dir="../crawler-temp-storage" \
            --workers=3 \
            --disable-javascript \
            --max-reqs-per-sec=5 \
            --timeout=15 \
            --max-visited-urls=10000 \
            --ignore-robots-txt=false
        continue-on-error: true

      - name: Check if report was generated
        id: check_report
        run: |
          if [ -f "./crawler-results/clover_docs_report.html" ]; then
            echo "report_exists=true" >> $GITHUB_OUTPUT
          else
            echo "report_exists=false" >> $GITHUB_OUTPUT
            echo "HTML report was not generated by the crawler."
          fi

      - name: Commit and push results
        if: steps.check_report.outputs.report_exists == 'true'
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add ./crawler-results/clover_docs_report.html
          if ! git diff --staged --quiet; then
            git commit -m "Weekly crawl results for docs.clover.com ($(date +'%Y-%m-%d'))"
            git push
          else
            echo "No changes in the report to commit."
          fi
